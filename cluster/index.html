<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
	<title>Computing Resources - UM SPH Department of Biostatistics</title>    
    <meta http-equiv="cache-control" content="no-cache" />
    <meta http-equiv="pragma" content="no-cache" />
	<link rel="stylesheet" type="text/css" href="/css/sph.css" />
	<link rel="stylesheet" type="text/css" href="/biostat/css/stylesheet.css" />
	<link rel="stylesheet" type="text/css" href="/css/print.css" media="print" />
	<link rel="stylesheet" type="text/css" href="/css/mobile.css" media="handheld" />
	<link rel="stylesheet" type="text/css" href="/biostat/computing/overrides.css" />
</head>
<body>
<!-- top blue banner //-->
<!--#include virtual="/inc/header.inc" -->
<!--bread crumbs//-->
<table cellspacing="0" cellpadding="0" width="100%">
  <tr> 
    <td class="breadcrumb"><a href="/" class="breadcrumbs" title="UM SPH Home">UM SPH Home</a> &gt; <a href="/biostat/" title="Department of Biostatistics">Department of Biostatistics</a> &gt; <a href="/biostat/computing" title="Biostatistics Computing Resources">Biostatistics Computing Resources</a> &gt; Cluster Documentation</td>
  </tr>
</table>
<!-- body area//-->
<table width="100%"  border="0" cellspacing="0" cellpadding="0">
  <tr valign="top">
    <td id="left">
	<!--#include virtual="/biostat/nav.html" -->
    <div id="facts">
    <h2>Biostatistics Facts and Figures:</h2>
    <ul>
    	<li><a href="http://www.sph.umich.edu/iscr/faculty/profile.cfm?uniqname=teraghu">Chair:Trivellore Raghunathan, Ph.D.</a></li>
        <li><a href="/iscr/faculty/dept.cfm?deptID=1">30 faculty</a></li>
        <li><a href="/biostat/students.html">152 students</a>: 67 Ph.D., 49 M.S., 2 M.P.H., 32 OJ/OC</li>
        <li><a href="/biostat/staff.html">70 staff</a>: 8 support staff, 60 research staff, 2 post-docs</li>
        <li><a href="/biostat/alumni.html">1100+ alumni</a></li>
        <li><a href="/biostat/research.html">$22M sponsored research</a></li>
    </ul>
	</div></td>
  <td id="content"><a name="content"></a>
  <!-- body content //-->
    <h1 id="dept_title">Department of Biostatistics <img src="/biostat/images/june08/blocks.gif" width="165" height="17" alt="" style="vertical-align:middle" /></h1>
    <h1>Bisotatistics Cluster Documentation</h1>

    <h3>Contents</h3>
    <ol id="toc">
      <li><a href="quickstart.html">Quick Start for new users</a></li>
      <li><a href="faq.html">Frequently Asked Questions</a></li>
      <li><a href="accounts_allocations.html">Accounts and Allocations</a></li>
      <li>
        <a href="#cluster_resources">Hardware/Software Resources</a>
        <ol>
          <li><a href="login_nodes.html">Login Nodes</a></li>
          <li><a href="#nodes">Cluster Nodes</a></li>
          <li><a href="#filesystems">Filesystems</a></li>
          <li><a href="software.html">Software Modules</a></li>
        </ol>
      </li>

      <li><a href="jobs.html">Jobs</a></li>
      <li>
        <a href="#slurm">SLURM Scheduler/Resource Manager</a>
        <ol>
          <li><a href="#sbatch">sbatch</a></li>
          <li><a href="#sarray">sarray</a></li>
          <li><a href="#squeue">squeue</a></li>
          <li><a href="#scancel">scancel</a></li>
          <li><a href="#sinfo">sinfo</a></li>
          <li><a href="#srun">srun</a></li>
          <li><a href="#salloc">salloc</a></li>
        </ol>
      </li>
      <li>
        <a href="#examples">Examples</a>
        <ol>
          <li><a href="#example_r_snow_socket">R with snow<em>(fall)</em> <em>type="SOCK"</em></a></li>
          <li><a href="#example_r_snow_mpi">R with snow<em>(fall)</em> <em>type="MPI"</em></a></li>
          <li><a href="#example_sas">SAS</a></li>
          <li><a href="#example_matlab">MATLAB</a></li>
        </ol>
      </li>
    </ol>

    <a name="cluster_resources"></a>
    <fieldset class="cluster" id="cluster_resources">
      <legend>Cluster Resources</legend>

      <a name="nodes"></a>
      <h3>Nodes</h3>
      <!--#include virtual="/biostat/computing/cluster/nodes.html" -->
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="filesystems"></a>
      <h3>Filesystems</h3>
      <!--#include virtual="/biostat/computing/cluster/filesystems.html" -->
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="modules"></a>
      <!--#include virtual="/biostat/computing/modules.html" -->
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="software"></a>
      <h3>Software Modules</h3>
      <!--#include virtual="/biostat/computing/cluster/modules.html" -->
      <a href="#" class="cluster_top">[back to top]</a>
    </fieldset>

    <a name="slurm"></a>
    <fieldset class="cluster" id="cluster_slurm">
      <legend>SLURM</legend>
      <p>We are using <a href="">SLURM (Simple Linux Utility for Resource Management)</a> for our resource manager and job scheduler. Below are several of the basic commands you will need to interact with the cluster.</p>

      <p>The commands for launching jobs in the cluster are <a href="#sbatch">sbatch</a>, <a href="#srun">srun</a>,
      <a href="#salloc">salloc</a>. The preferred and primary means of launching jobs in the cluster is with
      <a href="#sbatch">sbatch</a>.</p>

      <blockquote>
        <p>
          In a nutshell, sbatch and salloc allocate resources to the job, while srun launches parallel tasks across those resources.
          When invoked within a job allocation, srun will launch parallel tasks across some or all of the allocated resources.
          In that case, srun inherits by default the pertinent options of the sbatch or salloc which it runs under.  You can then
          (usually) provide srun different options which will override what it receives by default.  Each invocation of srun within
          a job is known as a job step. 
        </p>
        <p>
          srun can also be invoked outside of a job allocation.  In that case, srun requests resources,
          and when those resources are granted, launches tasks across those resources as a single job and job step. 
        </p>
      </blockquote>
      <div>Source: <cite><a href="http://groups.google.com/group/slurm-devel/msg/b8c9a2f51ce334a1">slurm-devel mailing list post</a></cite></div>
      
      <a name="sbatch"></a>
      <h4>sbatch</h4>
      <p>Submit a batch script to SLURM. This is the preferred and primary means of launching jobs in the cluster.</p>
      <p>sbatch submits a batch script to SLURM. The batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. The batch script may contain options preceded with "#SBATCH" before any executable commands in the script.</p>
      <p>sbatch exits immediately after the script is successfully transferred to the SLURM controller and assigned a SLURM job ID. The batch script is not necessarily granted resources immediately, it may sit in the queue of pending jobs for some time before its required resources become available.</p>
      <p>When the job allocation is finally granted for the batch script, SLURM runs a single copy of the batch script on the first node in the set of allocated nodes.</p>

      <pre>$ cat sample.txt 
#!/bin/sh

#SBATCH --mail-type=ALL
#SBATCH --mail-user=<uniqname>@umich.edu
#SBATCH --ntasks=5
#SBATCH --job-name=test

R CMD BATCH script.R

$ sbatch ./sample.txt
Submitted batch job 25618

$ cat slurm-25618.out
# any output to STDOUT would be in this file</pre>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="sarray"></a>
      <h4>sarray</h4>
      <p>This script is an attempt at providing the array job concept from PBS in SLURM.  You can run multiple jobs from a single batch script with the only difference in each job being the array index. The array index gives the script a means of changing parameters, loading differing data files or anything that requires a unique index.</p>

  <p>This script only accepts an batch script suitable for sbatch. The batch script must include the comment marker to indicate the range of array indexes. The comment is similiar to sbatch comments that must start in column zero of the script. The value can be a range separated by a dash ( -), a comma separated list of index numbers or both.</p>

<pre>#SARRAY --range=1-20</pre>

<pre>#SARRAY --range=1-10,12,14,16-20</pre>

<p>This would create jobs with the id appended to the end of your defined job name with the id within brackets such as; jobname[1], jobname[2] ... jobname[20].</p>
<p>The array index is passed back to the batch script as the environment variable $SLURM_ARRAYID for each iteration through the index(es) you provided via the --range option.</p>

<h5>Usage</h5>
<pre>$ sarray ./job.txt
$ sarray -J ./job.txt</pre>
<em><strong>-J</strong> Will maintain the jobname you specified and not attempt to append the array id</em>

<h5>Examples</h5>
<pre>$ cat array_job.txt
#!/bin/sh

#SBATCH --mail-type=ALL
#SBATCH --mail-user=uniqname@umich.edu
#SBATCH --time=1-0
#SBATCH --job-name=array_job_test

#SARRAY --range=1-10

srun R CMD BATCH script-${SLURM_ARRAYID}.R

$ sarray ./array_job.txt
Submitted batch job 12345
Submitted batch job 12346
Submitted batch job 12347
Submitted batch job 12348
Submitted batch job 12349
Submitted batch job 12350
Submitted batch job 12351
Submitted batch job 12352
Submitted batch job 12353
Submitted batch job 12354
$
</pre>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="squeue"></a>
      <h4>squeue</h4>
      <p>This command is used to view information about the slurm scheduling queue.</p>
 
      <p>To view you jobs in the queue regardless of their current state.</p>

      <pre>$ squeue -u $USER</pre>
      <a href="#" class="cluster_top">[back to top]</a>

      <h4>Job Status Codes</h4>
      <!--#include virtual="job_status_codes.html" -->
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="scancel"></a>
      <h4>scancel</h4>
      <p>Used to signal jobs or job steps that are under the control of Slurm.</p>

      <p><em>scancel</em> is used to signal or cancel jobs or job steps. An arbitrary number of jobs or job steps may be signaled using job specification filters or a space separated list of specific job and/or job step IDs. A job or job step can only be signaled by the owner of that job or user root. If an attempt is made by an unauthorized user to signal a job or job step, an error message will be printed and the job will not be signaled.</p>

      <pre>$ squeue 
JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)
29908 biostat-d     bash  schelcj   R       0:05      2 cn[001-002]
$ scancel 29908
$ squeue
JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)
$</pre>

      <p>Here we see our jobid is 29908 then we cancel that job with the scancel command.</p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="sinfo"></a>
      <h4>sinfo</h4>
      <p>View information about SLURM nodes and partitions.</p>

      <pre>$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
biostat-*    up   infinite      5  alloc cn[008-012]
biostat-*    up   infinite     10   idle andoria,cn[001-007],tellar,vulcan
biostat-b    up   infinite      3   idle andoria,tellar,vulcan</pre>

      <p>This shows that nodes cn008 through cn012 have jobs allocated to them but they can still accpet more work. The rest of the cluster is currently idle. If you wanted to run an MPI job that used 40 cores then you could choose to use cn001 and cn002 since these nodes have 12 cores apiece.</p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="srun"></a>
      <h4>srun</h4>
      <p>Used to launch a jobs in the cluster</p>

      <p>The following example runs the /bin/hostname command on at least two nodes in the cluster. STDOUT/STDERR return to your terminal.</p>

      <pre> $ srun --nodes=2 hostname
cn001
cn002</pre>

      <p>You can specify the number of cpu's to use per job with the <em>--cpus-per-task</em> option.</p>

      <pre>$ srun --nodes=2 --cpus-per-task=3 R CMD BATCH script.R</pre>

      <p>All SAS jobs should be run in the <em>biostat-sas</em> partition to meet license restrictions.</p>

      <pre>$ srun --partition=biostat-sas some_sas_script.sas</pre>

      <p>Provided there are available licenses the job will be launched according the the normal allocation process. If the license pool has been exhausted then the job will remain in the pending state until a license is freed for use.</p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="salloc"></a>
      <h4>salloc</h4>
      <p>Obtain an interactive SLURM job allocation (a set of nodes), execute a command, and then release the allocation when the command is finished.</p>

      <p>This just requests an allocation of cluster resources and gives you an interactive shell (when you exit that shell the allocation is relinquished and all jobs running are cancelled.) For example</p>

      <pre>$ salloc -N2
salloc: Granted job allocation 29903
$ srun hostname
cn002
cn001
$</pre>

      <p>Here we have requested two nodes be allocated then we run <em>/bin/hostname</em> to see what nodes we were allocated. Any command we run within this allocation is run in parallel across the number of nodes we requested, in this case 2. So for instance if you had an R script that generated its own unique dataset for each invocation you could have ran that script across two nodes. For example;</p>

      <pre>$ salloc -N2
salloc: Grant job allocation 12345
$ srun R CMD BATCH script.R &
$ srun R CMD BATCH script.R &
$ srun R CMD BATCH script.R &
$</pre>

      <p>In this example you have a two node allocation with each call to srun using a single core on both machines. Each call to srun launches the same R script on each node and the & sign backgrounds the process so that you can issue more srun commands. This example is now running the R script 6 times across two nodes.</p>

      <pre>$ env|grep SLURM
SLURM_NODELIST=cn[001-002]
SLURM_NNODES=2
SLURM_JOBID=29903
SLURM_TASKS_PER_NODE=2(x2)
SLURM_JOB_ID=29903
SLURM_JOB_NODELIST=cn[001-002]
SLURM_JOB_CPUS_PER_NODE=2(x2)
SLURM_JOB_NUM_NODES=2</pre>

      <p>While within the interactive allocation you have several environmental varilables available to you that describe the allocation. Having things like the JOB_ID available make it possible to use the <em>--dependency</em> option to srun.</p>
      <a href="#" class="cluster_top">[back to top]</a>
    </fieldset>

    <a name="examples"></a>
    <fieldset class="cluster" id="cluster_examples">
      <legend>Examples</legend>

      <h3>R with snow(fall)</h3>

      <p>The snow and snowfall packages can be used in R to simplify the process of writing parallel R scripts. These packages work by creating separate R processes (slaves/nodes) and then sending jobs to those nodes in parellel. You have a couple different options for communication between these nodes. The options are communicationare <a href="http://en.wikipedia.org/wiki/Unix_domain_socket">SOCKETS</a>, <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>, and <a href="http://en.wikipedia.org/wiki/Parallel_Virtual_Machine">PVM</a>. The biostatistics cluster does not implement PVM leaving you with a choice between MPI and SOCKETS. The basic difference between the two is that SOCKETS only allow communication within a single host and MPI communicates across the network. When you submit a R job to the cluster using snow/snowfall you should consider how many R cluster nodes you wish to start as this will help you decide which type you need to use. You can think of the snow/snowfall nodes as 1:1 with cores and nodes, each node you start will use a single core. If you need to start more slaves then a single cluster node has cores then you will need to use MPI so those R slaves can communicate across the network. See the table above for how many cores each cluster node has. Here are some examples of sbatch batch file for each type:</p>

      <a name="example_r_snow_socket"></a>
      <h4>Snow/Snowfall node type SOCK</h4>
      <ul>
        <li>snow cluster init sample: <em>makeCluster(12, type="SOCK")</em></li>
        <li>snowfall cluster init sample: <em>sfInit(parallel=T,cpus=12,type="SOCK")</em></li>
      </ul>

<pre>$ cat sock-job.txt
#!/bin/sh
#SBATCH --mail-type=ALL
#SBATCH --mail-user=uniqname@umich.edu
#SBATCH --cpus-per-task=12
#SBATCH --time=1-0
#SBATCH --job-name=test_r_socket_job
#SBATCH --mem=2000

srun R CMD BATCH ./script.R</pre>

      <p>This batch script tells the scheduler that you would like this job to allocate 12 cpus(cores) per task, so 12 cores for 12 snow/snowfall nodes, that it will run for one day and requires 2GB of memory.</p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="example_r_snow_mpi"></a>
      <h4>Snow/Snowfall node type MPI</h4>
      <ul>
        <li>snow cluster init sample: <em>makeCluster(100, type="MPI")</em></li>
        <li>snowfall cluster init sample: <em>sfInit(parallel=T,cpus=100,type="MPI")</em></li>
      </ul>

<pre>$ cat mpi-job.txt
#!/bin/sh
#SBATCH --mail-type=ALL
#SBATCH --mail-user=uniqname@umich.edu
#SBATCH --ntasks=100
#SBATCH --time=1-0
#SBATCH --job-name=test_r_socket_job
#SBATCH --mem=2000

R CMD BATCH ./script.R</pre>

      <p>This batch script tells the scheduler that you would like to allocate 100 cpus(cores) to this single task, giving you 100 snow/snowfall nodes, that it will run for one day and requires 2GB of memory.</p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="example_sas"></a>
      <h4>SAS</h4>

<pre>$ module load sas
$ cat sas-job.txt
#!/bin/sh
#SBATCH --mail-type=ALL
#SBATCH --mail-user=uniqname@umich.edu
#SBATCH --partition=biostat-sas
#SBATCH --time=1-0
#SBATCH --job-name=test_sas_job
#SBATCH --mem=2000

sas file.sas</pre>

      <p>First you must load the SAS module with the <em>module load sas</em> command. Then the batch script tells the scheduler that your job will use the default single core, request 2gb of memory, and request that the job be run in the <em>biostat-sas</em> partition.</p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="example_matlab"></a>
      <h4>MATLAB</h4>

<pre>$ module load matlab
$ cat matlab-job.txt
#!/bin/sh
#SBATCH --mail-type=ALL
#SBATCH --mail-user=uniqname@umich.edu
#SBATCH --license=MATLAB
#SBATCH --time=1-0
#SBATCH --job-name=test_matlab_job
#SBATCH --mem=4000

matlab -nodisplay -nodesktop -r matlab_file</pre>

      <p>First you must load the MATLAB module with the <em>module load matlab</em> command. Then the batch script tells teh scheduler that your job will use teh default single core, request 4gb of memory, and request a single MATLAB license.</p>
      <a href="#" class="cluster_top">[back to top]</a>
    </fieldset>
  </tr>
</table>
<!-- footer //-->
<!--#include virtual="/inc/footer.html" -->
</body>
</html>
