<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
	<title>Computing Resources - UM SPH Department of Biostatistics</title>    
    <meta http-equiv="cache-control" content="no-cache" />
    <meta http-equiv="pragma" content="no-cache" />
	<link rel="stylesheet" type="text/css" href="/css/sph.css" />
	<link rel="stylesheet" type="text/css" href="/biostat/css/stylesheet.css" />
	<link rel="stylesheet" type="text/css" href="/css/print.css" media="print" />
	<link rel="stylesheet" type="text/css" href="/css/mobile.css" media="handheld" />
	<link rel="stylesheet" type="text/css" href="/biostat/computing/overrides.css" />
</head>
<body>
<!-- top blue banner //-->
<!--#include virtual="/inc/header.inc" -->
<!--bread crumbs//-->
<table cellspacing="0" cellpadding="0" width="100%">
  <tr> 
    <td class="breadcrumb"><a href="/" class="breadcrumbs" title="UM SPH Home">UM SPH Home</a> &gt; <a href="/biostat/" title="Department of Biostatistics">Department of Biostatistics</a> &gt; <a href="/biostat/computing" title="Biostatistics Computing Resources">>Biostatistics Computing Resources</a> &gt; Cluster Documentation</td>
  </tr>
</table>
<!-- body area//-->
<table width="100%"  border="0" cellspacing="0" cellpadding="0">
  <tr valign="top">
    <td id="left">
	<!--#include virtual="/biostat/nav.html" -->
    <div id="facts">
    <h2>Biostatistics Facts and Figures:</h2>
    <ul>
    	<li><a href="http://www.sph.umich.edu/iscr/faculty/profile.cfm?uniqname=teraghu">Chair:Trivellore Raghunathan, Ph.D.</a></li>
        <li><a href="/iscr/faculty/dept.cfm?deptID=1">30 faculty</a></li>
        <li><a href="/biostat/students.html">152 students</a>: 67 Ph.D., 49 M.S., 2 M.P.H., 32 OJ/OC</li>
        <li><a href="/biostat/staff.html">70 staff</a>: 8 support staff, 60 research staff, 2 post-docs</li>
        <li><a href="/biostat/alumni.html">1100+ alumni</a></li>
        <li><a href="/biostat/research.html">$22M sponsored research</a></li>
    </ul>
	</div></td>
  <td id="content"><a name="content"></a>
  <!-- body content //-->
    <h1 id="dept_title">Department of Biostatistics <img src="/biostat/images/june08/blocks.gif" width="165" height="17" alt="" style="vertical-align:middle" /></h1>
    <h1>Bisotatistics Cluster Documentation</h1>

    <h3>Contents</h3>
    <ol id="toc">
      <li><a href="cluster_quickstart.html">Quick Start for new users</a></li>
      <li>
        <a href="#accounts">Accounts and Allocations</a>
        <ol>
          <li><a href="#allocation">Allocation</a></li>
          <li><a href="#passwords">Passwords</a></li>
          <li><a href="#access">Access</a></li>
        </ol>
      </li>
      <li>
        <a href="#cluster_resources">Hardware/Software Resources</a>
        <ol>
          <li><a href="#nodes">Cluster Nodes</a></li>
          <li><a href="#modules">Environment Modules</a></li>
          <li><a href="#software">Software Modules</a></li>
        </ol>
      </li>

      <li><a href="#filesystems">Filesystems</a></li>
      <li>
        <a href="#jobs">Jobs</a>
        <ol>
          <li><a href="#queue_policy">Queueing Policy</a></li>
          <li><a href="#limitations">Job Limitations</a></li>
          <li><a href="#partitions">Partitions</a></li>
        </ol>
      </li>
      <li>
        <a href="#slurm">SLURM Scheduler/Resource Manager</a>
        <ol>
          <li><a href="#srun">srun</a></li>
          <li><a href="#sbatch">sbatch</a></li>
          <li><a href="#squeue">squeue</a></li>
          <li><a href="#scancel">scancel</a></li>
          <li><a href="#sinfo">sinfo</a></li>
        </ol>
      </li>
      <li>
        <a href="#examples">Examples</a>
        <ol>
          <li><a href="#example_r_snow_socket">R with snow<em>(fall)</em> <em>type="SOCK"</em></a></li>
          <li><a href="#example_r_snow_mpi">R with snow<em>(fall)</em> <em>type="MPI"</em></a></li>
        </ol>
      </li>
      <li><a href="#resources">Resources</a></li>
    </ol>


    <a name="accounts"></a>
    <fieldset class="cluster" id="cluster_accounts">
      <legend>Accounts / Allocation</legend>
      <a name="allocation"></a>
      <h3>Account Allocation</h3>
      <p>
        Cluster accounts are available to all biostat faculty and students.
        There is currently no cost associated with an account allocation
        but that will change in the future. Faculty may request an account
        directly by emailing
        <a href="mailto:sph-biostat-help@umich.edu">sph-biostat-help@umich.edu</a>.
        Students will have to have their advisor request the account for them. Your
        login username will be your umich uniqname and your password will be your
        UMICH IFS Kerberos password.
      </p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="passwords"></a>
      <h3>Passwords</h3>
      <p>
        Your kerberos password for your uniqname login is what you will need to login to the cluster.
      </p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="access"></a>
      <h3>Access Details</h3>
      <p> All access to the cluster is over ssh within the umich network.</p>

      <dl>
        <dt>Hostname:</dt>
        <dd><em>biostat-login.sph.umich.edu</em>
        <dt>Username:</dt>
        <dd><em>uniqname</em></dd>
        <dt>Password:</dt>
        <dd><em>Your UMICH IFS/Kerberos password</em></dd>
      </dl>
      <a href="#" class="cluster_top">[back to top]</a>
    </fieldset>

    <a name="cluster_resources"></a>
    <fieldset class="cluster" id="cluster_resources">
      <legend>Cluster Resources</legend>

      <a name="nodes"></a>
      <h3>Nodes</h3>
      <table>
        <thead>
          <tr>
            <th>Node #</th>
            <th>Hostname</th>
            <th>Processor</th>
            <th>Processor Clock Speed</th>
            <th>Physical Cores</th>
            <th>Logical Cores</th>
            <th>Memory</th>
            <th>Diskspace</th>
            <th>Slurm Features</th>
            <th>Slurm Paritions</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>1-12</td>
            <td>cn0[01-12]</td>
            <td>Intel Xeon X5660</td>
            <td>2.80GHz</td>
            <td>12</td>
            <td>24</td>
            <td>32GB</td>
            <td>85GB</td>
            <td>rack-15W-E, intel</td>
            <td>biostat-default</td>
          </tr>
          <tr>
            <td>13-16</td>
            <td>cn0[13-16]</td>
            <td>Intel Xeon X5660</td>
            <td>2.80GHz</td>
            <td>12</td>
            <td>24</td>
            <td>32GB</td>
            <td>85GB</td>
            <td>rack-15W-F, intel</td>
            <td>biostat-default, contrib-bnan</td>
          </tr>
          <tr>
            <td>17</td>
            <td>tellar</td>
            <td>AMD Opteron Processor 6174</td>
            <td>3.0GHz</td>
            <td>48</td>
            <td>48</td>
            <td>128Gb</td>
            <td>256Gb</td>
            <td>rack-15W-E, amd</td>
            <td>biostat-default, contrib-mbb</td>
          </tr>
          <tr>
          <td>18</td>
            <td>andoria</td>
            <td>Intel Xeon X5680</td>
            <td>3.33Ghz</td>
            <td>12</td>
            <td>24</td>
            <td>144Gb</td>
            <td>500Gb</td>
            <td>rack-15W-E, intel</td>
            <td>biostat-default, biostat-bigmem</td>
          </tr>
          <tr>
            <td>19</td>
            <td>vulcan</td>
            <td>Intel Xeon X5680</td>
            <td>3.33GHz</td>
            <td>12</td>
            <td>24</td>
            <td>144Gb</td>
            <td>500Gb</td>
            <td>rack-15W-E, intel</td>
            <td>biostat-default, biostat-bigmem</td>
          </tr>
        </tbody>
      </table>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="modules"></a>
      <!--#include virtual="/biostat/computing/modules.html" -->
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="software"></a>
      <h3>Software Modules</h3>
      <!--#include virtual="/biostat/computing/cluster_modules.html" -->
      <a href="#" class="cluster_top">[back to top]</a>
    </fieldset>

    <a name="filesystems"></a>
    <fieldset class="cluster" id="cluster_filesystems">
      <legend>Filesystems</legend>
      <dl>
        <dt>/home/<em>uniqname</em></dt>
        <dd>Home directories are available on all compute nodes across the cluster over NFS. There is a 50GB quota for all home direcotries. All home directories are backed up nightly.</dd>
        <dt>/tmp</dt>
        <dd>Each compute node has local diskspace in /tmp available for your use. Writing to this directory saves you from writing over the network via NFS. This directory is not backed up and will be cleaned out on a regular basis.</dd>
        <dt>/afs</dt>
        <dd>The afs space available is the UMICH IFS cell. There is a symlink in your home directory /home/<em>uniqname</em>/ifs that is your IFS user home directory. IFS is only available on the login nodes not on the compute nodes. This means that you will need to copy any scripts/data from your ifs space to your home directory that are required to run your job.</dd>
      </dl>
      <a href="#" class="cluster_top">[back to top]</a>
    </fieldset>

    <a name="jobs"></a>
    <fieldset class="cluster" id="cluster_jobs">
      <legend>Jobs</legend>

      <a name="queue_policy"></a>
      <h3>Queueing Policy</h3>
      <p>
        All jobs are scheduled on a <a href="http://en.wikipedia.org/wiki/FIFO">FIFO</a> basis.
        There are no max proc limitations so if there is an idle resource for your job it
        will be allocated to you.
      </p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="limitations"></a>
      <h3>Limitations</h3>
        <ul>
          <li>Maximum wallclock time of 28 days for all jobs.</li>
          <li>Maximum of 100 jobs in the queue regardless of state (ie; running, pending) per user.</li>
          <li>Unless requested each job defaults to one cpu core and 1.3GB of memory per job. You can request more with various options to salloc/srun/sbatch.</li>
          <li>Time argument <em>(--time=)</em> is required otherwise default time of 10 minutes is applied to your job.</li>
        </ul>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="partitions"></a>
      <h3>Partitions</h3>
      <!--#include virtual="/biostat/computing/cluster_partitions.html" -->
      <a href="#" class="cluster_top">[back to top]</a>
    </fieldset>

    <a name="slurm"></a>
    <fieldset class="cluster" id="cluster_slurm">
      <legend>SLURM</legend>
      <p>We are using <a href="">SLURM (Simple Linux Utility for Resource Management)</a> for our resource manager and job scheduler. Below are several of the basic commands you will need to interact with the cluster.</p>

      <a name="srun"></a>
      <h4>srun</h4>
      <p>Used to launch a jobs in the cluster</p>

      <p>The following example runs the /bin/hostname command on at least two nodes in the cluster. STDOUT/STDERR return to your terminal.</p>

      <pre> $ srun --nodes=2 hostname
cn001
cn002</pre>

      <p>You can specify the number of cpu's to use per job with the <em>--cpus-per-task</em> option.</p>

      <pre>$ srun --nodes=2 --cpus-per-task=3 R CMD BATCH script.R</pre>

      <p>To request a license for software that has a limited number of licenses you will need to use the --license option to srun.</p>

      <pre>$ srun --license=SAS some_sas_script.sas</pre>

      <p>Provided there are available licenses the job will be launched according the the normal allocation process. If the license pool has been exhausted then the job will remain in the pending state until a license is freed for use.</p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="sbatch"></a>
      <h4>sbatch</h4>
      <p>Submit a batch script to SLURM.</p>
      <p>sbatch submits a batch script to SLURM. The batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. The batch script may contain options preceded with "#SBATCH" before any executable commands in the script.</p>
      <p>sbatch exits immediately after the script is successfully transferred to the SLURM controller and assigned a SLURM job ID. The batch script is not necessarily granted resources immediately, it may sit in the queue of pending jobs for some time before its required resources become available.</p>
      <p>When the job allocation is finally granted for the batch script, SLURM runs a single copy of the batch script on the first node in the set of allocated nodes.</p>

      <pre>$ cat sample.txt 
#!/bin/sh

#SBATCH --mail-type=ALL
#SBATCH --mail-user=<uniqname>@umich.edu
#SBATCH --ntasks=5
#SBATCH --job-name=test

R CMD BATCH script.R

$ sbatch ./sample.txt
Submitted batch job 25618

$ cat slurm-25618.out
# any output to STDOUT would be in this file</pre>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="squeue"></a>
      <h4>squeue</h4>
      <p>This command is used to view information about the slurm scheduling queue.</p>
 
      <p>To view you jobs in the queue regardless of their current state.</p>

      <pre>$ squeue -u $USER</pre>
      <a href="#" class="cluster_top">[back to top]</a>

      <h4>Job Status Codes</h4>
      <table>
        <thead>
          <tr>
            <th>Code</th>
            <th>State</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>CA</td>
            <td>CANCELLED</td>
            <td>Job  was explicitly cancelled by the user or system administrator.  The job may or may  not  have  been initiated. </td>
          </tr>
          <tr>
            <td>CD</td>
            <td>COMPLETED</td>
            <td>Job has terminated all processes on all nodes.</td>
          </tr>
          <tr>
            <td>CF</td>
            <td>CONFIGURING</td>
            <td>Job  has  been allocated resources, but are waiting for them to become ready for use (e.g. booting).</td>
          </tr>
          <tr>
            <td>CG</td>
            <td>COMPLETING</td>
            <td>Job is in the process of completing. Some processes on some nodes may still be active.</td>
          </tr>
          <tr>
            <td>F</td>
            <td>FAILED</td>
            <td>Job  terminated  with  non-zero  exit code or other failure condition.</td>
          </tr>
          <tr>
            <td>NF</td>
            <td>NODE_FAIL</td>
            <td>Job terminated due to failure of one or more  allo‐ cated nodes.</td>
          </tr>
          <tr>
            <td>PD</td>
            <td>PENDING</td>
            <td>Job is awaiting resource allocation.</td>
          </tr>
          <tr>
            <td>R</td>
            <td>RUNNING</td>
            <td>Job currently has an allocation.</td>
          </tr>
          <tr>
            <td>S</td>
            <td>SUSPENDED</td>
            <td>Job  has an allocation, but execution has been suspended.</td>
          </tr>
          <tr>
            <td>TO</td>
            <td>TIMEOUT</td>
            <td>Job terminated upon reaching its time limit.</td>
          </tr>
        </tbody>
      </table>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="scancel"></a>
      <h4>scancel</h4>
      <p>Used to signal jobs or job steps that are under the control of Slurm.</p>

      <p><em>scancel</em> is used to signal or cancel jobs or job steps. An arbitrary number of jobs or job steps may be signaled using job specification filters or a space separated list of specific job and/or job step IDs. A job or job step can only be signaled by the owner of that job or user root. If an attempt is made by an unauthorized user to signal a job or job step, an error message will be printed and the job will not be signaled.</p>

      <pre>$ squeue 
JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)
29908 biostat-d     bash  schelcj   R       0:05      2 cn[001-002]
$ scancel 29908
$ squeue
JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)
$</pre>

      <p>Here we see our jobid is 29908 then we cancel that job with the scancel command.</p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="sinfo"></a>
      <h4>sinfo</h4>
      <p>View information about SLURM nodes and partitions.</p>

      <pre>$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
biostat-*    up   infinite      5  alloc cn[008-012]
biostat-*    up   infinite     10   idle andoria,cn[001-007],tellar,vulcan
biostat-b    up   infinite      3   idle andoria,tellar,vulcan</pre>

      <p>This shows that nodes cn008 through cn012 have jobs allocated to them but they can still accpet more work. The rest of the cluster is currently idle. If you wanted to run and MPI job that used 40 cores then you could choose to use cn001 and cn002 since these nodes have 12 cores apiece.</p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="salloc"></a>
      <h4>salloc</h4>
      <p>Obtain an interactive SLURM job allocation (a set of nodes), execute a command, and then release the allocation when the command is finished.</p>

      <p>This just requests an allocation of cluster resources and gives you an interactive shell (when you exit that shell the allocation is relinquished and all jobs running are cancelled.) For example</p>

      <pre>$ salloc -N2
salloc: Granted job allocation 29903
$ srun hostname
cn002
cn001
$</pre>

      <p>Here we have requested two nodes be allocated then we run <em>/bin/hostname</em> to see what nodes we were allocated. Any command we run within this allocation is run in parallel across the number of nodes we requested, in this case 2. So for instance if you had an R script that generated its own unique dataset for each invocation you could have ran that script across two nodes. For example;</p>

      <pre>$ salloc -N2
salloc: Grant job allocation 12345
$ srun R CMD BATCH script.R &
$ srun R CMD BATCH script.R &
$ srun R CMD BATCH script.R &
$</pre>

      <p>In this example you have a two node allocation with each call to srun using a single core on both machines. Each call to srun launches the same R script on each node and the & sign backgrounds the process so that you can issue more srun commands. This example is now running the R script 6 times across two nodes.</p>

      <pre>$ env|grep SLURM
SLURM_NODELIST=cn[001-002]
SLURM_NNODES=2
SLURM_JOBID=29903
SLURM_TASKS_PER_NODE=2(x2)
SLURM_JOB_ID=29903
SLURM_JOB_NODELIST=cn[001-002]
SLURM_JOB_CPUS_PER_NODE=2(x2)
SLURM_JOB_NUM_NODES=2</pre>

      <p>While within the interactive allocation you have several environmental varilables available to you that describe the allocation. Having things like the JOB_ID available make it possible to use the <em>--dependency</em> option to srun.</p>
      <a href="#" class="cluster_top">[back to top]</a>
    </fieldset>

    <a name="examples"></a>
    <fieldset class="cluster" id="cluster_examples">
      <legend>Examples</legend>

      <h3>R with snow(fall)</h3>

      <p>The snow and snowfall packages can be used in R to simplify the process of writing parallel R scripts. These packages work by creating separate R processes (slaves/nodes) and then sending jobs to those nodes in parellel. You have a couple different options for communication between these nodes. The options are communicationare <a href="http://en.wikipedia.org/wiki/Unix_domain_socket">SOCKETS</a>, <a href="http://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>, and <a href="http://en.wikipedia.org/wiki/Parallel_Virtual_Machine">PVM</a>. The biostatistics cluster does not implement PVM leaving you with a choice between MPI and SOCKETS. The basic difference between the two is that SOCKETS only allow communication within a single host and MPI communicates across the network. When you submit a R job to the cluster using snow/snowfall you should consider how many R cluster nodes you wish to start as this will help you decide which type you need to use. You can think of the snow/snowfall nodes as 1:1 with cores and nodes, each node you start will use a single core. If you need to start more slaves then a single cluster node has cores then you will need to use MPI so those R slaves can communicate across the network. See the table above for how many cores each cluster node has. Here are some examples of sbatch batch file for each type:</p>

      <a name="example_r_snow_socket"></a>
      <h4>Snow/Snowfall node type SOCK</h4>
      <ul>
        <li>snow cluster init sample: <em>makeCluster(12, type="SOCK")</em></li>
        <li>snowfall cluster init sample: <em>sfInit(parallel=T,cpus=12,type="SOCK")</em></li>
      </ul>

<pre>$ cat sock-job.txt
#!/bin/sh
#SBATCH --mail-type=ALL
#SBATCH --mail-user=uniqname@umich.edu
#SBATCH --cpus-per-task=12
#SBATCH --time=1-0
#SBATCH --job-name=test_r_socket_job
#SBATCH --mem=2000

srun R CMD BATCH ./script.R</pre>

      <p>This batch script tells the scheduler that you would like this job to allocate 12 cpus(cores) per task, so 12 cores for 12 snow/snowfall nodes, that it will run for one day and requires 2GB of memory.</p>
      <a href="#" class="cluster_top">[back to top]</a>

      <a name="example_r_snow_mpi"></a>
      <h4>Snow/Snowfall node type MPI</h4>
      <ul>
        <li>snow cluster init sample: <em>makeCluster(100, type="MPI")</em></li>
        <li>snowfall cluster init sample: <em>sfInit(parallel=T,cpus=100,type="MPI")</em></li>
      </ul>

<pre>$ cat mpi-job.txt
#!/bin/sh
#SBATCH --mail-type=ALL
#SBATCH --mail-user=uniqname@umich.edu
#SBATCH --ntasks=100
#SBATCH --time=1-0
#SBATCH --job-name=test_r_socket_job
#SBATCH --mem=2000

R CMD BATCH ./script.R</pre>

      <p>This batch script tells the scheduler that you would like to allocate 100 cpus(cores) to this single task, giving you 100 snow/snowfall nodes, that it will run for one day and requires 2GB of memory.</p>
      <a href="#" class="cluster_top">[back to top]</a>
    </fieldset>

    <a name="resources"></a>
    <fieldset class="cluster" id="resources">
      <legend>Resources</legend>
      <h3>Departmental Documentation</h3>
      <ul>
        <li><a href="/biostat/computing/compute_resource_for_biostatistics.pdf">Cluster Intro Brown Bag Notes</a>
        <li><a href="https://www.bio.sph.umich.edu/ganglia/">Cluster Status</a>
      </ul>

      <h3>Utils</h3>
      <ul>
        <li><a href="https://accounts.itcs.umich.edu/password/">UMICH ITS Password Reset Utility</a></li>
      </ul>

      <h3>Slurm Documentation</h3>
      <ul>
        <li><a href="https://computing.llnl.gov/linux/slurm/quickstart.html">Slurm Quickstart Guide</a></li>
        <li><a href="https://computing.llnl.gov/linux/slurm/mpi_guide.html#open_mpi">Slurm OpenMPI Jobs</a></li>
        <li><a href="https://computing.llnl.gov/linux/slurm/slurm.html">Slurm Project</a></li>
      </ul>
      
      <h3>Slurm man pages</h3>
      <ul>
        <li><a href="http://manpages.ubuntu.com/manpages/lucid/man1/srun.1.html">srun</a></li>
        <li><a href="http://manpages.ubuntu.com/manpages/lucid/man1/sbatch.1.html">sbatch</a></li>
        <li><a href="http://manpages.ubuntu.com/manpages/lucid/man1/squeue.1.html">squeue</a></li>
        <li><a href="http://manpages.ubuntu.com/manpages/lucid/man1/scancel.1.html">scancel</a></li>
        <li><a href="http://manpages.ubuntu.com/manpages/lucid/man1/sinfo.1.html">sinfo</a></li>
        <li><a href="http://manpages.ubuntu.com/manpages/lucid/man1/salloc.1.html">salloc</a></li>
      </ul>

      <h3>R</h3>
      <ul>
        <li><a href="http://www.sfu.ca/~sblay/R/snow.html">Snow</a></li>
        <li><a href="http://docs.google.com/viewer?a=v&q=cache:ZcrRMWgNs18J:cran.r-project.org/web/packages/snowfall/vignettes/snowfall.pdf+R+snowfall&hl=en&gl=us&pid=bl&srcid=ADGEESiFpPILuTD4x7AVQECq4TAympKaRGRHo62-RqvxjYZirRdukjp7fyYd1R_q80vD3tEcHts29OyBREhqIXDtdX9Y7deCfF5OjXzY7Bw-PoIm1-pdJc1Xv70L6P7kSdXg2RHUspX4&sig=AHIEtbRf_e1D-6f9a4-Jg64gyo6XBmK0-Q">Snowfall</a></li>
      </ul>
      <a href="#" class="cluster_top">[back to top]</a>
    </fieldset>

  </tr>
</table>
<!-- footer //-->
<!--#include virtual="/inc/footer.html" -->
</body>
</html>
